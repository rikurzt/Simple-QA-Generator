import streamlit as st
import tempfile
import os
import json
from openai import OpenAI
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    CSVLoader,
    EverNoteLoader,
    PyMuPDFLoader,
    TextLoader,
    UnstructuredEmailLoader,
    UnstructuredEPubLoader,
    UnstructuredHTMLLoader,
    UnstructuredMarkdownLoader,
    UnstructuredODTLoader,
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader,
)
from typing import List

# OpenAIå®¢æˆ·ç«¯é…ç½®ï¼ˆå°‡åœ¨é‹è¡Œæ™‚æ ¹æ“šç”¨æˆ¶è¨­å®šåˆå§‹åŒ–ï¼‰
client = None

# Document loaders mapping
LOADER_MAPPING = {
    ".csv": (CSVLoader, {}),
    ".doc": (UnstructuredWordDocumentLoader, {}),
    ".docx": (UnstructuredWordDocumentLoader, {}),
    ".enex": (EverNoteLoader, {}),
    ".eml": (UnstructuredEmailLoader, {}),
    ".epub": (UnstructuredEPubLoader, {}),
    ".html": (UnstructuredHTMLLoader, {}),
    ".md": (UnstructuredMarkdownLoader, {}),
    ".odt": (UnstructuredODTLoader, {}),
    ".pdf": (PyMuPDFLoader, {}),
    ".ppt": (UnstructuredPowerPointLoader, {}),
    ".pptx": (UnstructuredPowerPointLoader, {}),
    ".txt": (TextLoader, {"encoding": "utf8"}),
}

def init_openai_client():
    """åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯"""
    global client
    try:
        api_key = st.session_state.get('api_key', '')
        base_url = st.session_state.get('base_url', 'https://api.openai.com/v1')
        
        if not api_key:
            st.error("è«‹å…ˆè¨­å®šAPI Key")
            return False
            
        client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        return True
    except Exception as e:
        st.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯å¤±æ•—: {e}")
        return False

def get_completion(prompt, model=None, temperature=None, max_tokens=None):
    """ç²å–æ¨¡å‹çš„éŸ¿æ‡‰"""
    global client
    
    # å¦‚æœå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œå˜—è©¦åˆå§‹åŒ–
    if client is None:
        if not init_openai_client():
            return None
    
    # ä½¿ç”¨ç”¨æˆ¶è¨­å®šçš„åƒæ•¸æˆ–é»˜èªå€¼
    model = model or st.session_state.get('model_name', 'gpt-4.1-nano')
    temperature = temperature if temperature is not None else st.session_state.get('temperature', 0.1)
    max_tokens = max_tokens or st.session_state.get('max_tokens', 4096)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"èª¿ç”¨APIæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def get_default_qa_prompt():
    """ç²å–é»˜èªçš„QAç”Ÿæˆæç¤ºè©"""
    return """åŸºæ–¼ä»¥ä¸‹çµ¦å®šçš„æ–‡æœ¬ï¼Œç”Ÿæˆå¤šçµ„é«˜è³ªé‡çš„å•ç­”å°ã€‚è«‹éµå¾ªä»¥ä¸‹æŒ‡å—ï¼š

1. å•é¡Œéƒ¨åˆ†ï¼š
- ç‚ºä¸åŒçš„ä¸»é¡Œå’Œæ¦‚å¿µå‰µå»ºå¤šå€‹å•ç­”å°
- æ¯å€‹å•é¡Œæ‡‰è€ƒæ…®ç”¨æˆ¶å¯èƒ½çš„å¤šç¨®å•æ³•ï¼Œä¾‹å¦‚ï¼š
- ç›´æ¥è©¢å•ï¼ˆå¦‚"ä»€éº¼æ˜¯...ï¼Ÿ"ï¼‰
- è«‹æ±‚ç¢ºèªï¼ˆå¦‚"æ˜¯å¦å¯ä»¥èªª...ï¼Ÿ"ï¼‰
- å°‹æ±‚è§£é‡‹ï¼ˆå¦‚"è«‹è§£é‡‹ä¸€ä¸‹...çš„å«ç¾©ã€‚"ï¼‰
- å‡è¨­æ€§å•é¡Œï¼ˆå¦‚"å¦‚æœ...æœƒæ€æ¨£ï¼Ÿ"ï¼‰
- ä¾‹å­è«‹æ±‚ï¼ˆå¦‚"èƒ½å¦èˆ‰å€‹ä¾‹å­èªªæ˜...ï¼Ÿ"ï¼‰
- å•é¡Œæ‡‰æ¶µè“‹æ–‡æœ¬ä¸­çš„é—œéµä¿¡æ¯ã€ä¸»è¦æ¦‚å¿µå’Œç´°ç¯€ï¼Œç¢ºä¿ä¸éºæ¼é‡è¦å…§å®¹ã€‚

2. ç­”æ¡ˆéƒ¨åˆ†ï¼š
- æä¾›ä¸€å€‹å…¨é¢ã€ä¿¡æ¯è±å¯Œçš„ç­”æ¡ˆï¼Œæ¶µè“‹å•é¡Œçš„æ‰€æœ‰å¯èƒ½è§’åº¦ï¼Œç¢ºä¿é‚è¼¯é€£è²«ã€‚
- ç­”æ¡ˆæ‡‰ç›´æ¥åŸºæ–¼çµ¦å®šæ–‡æœ¬ï¼Œç¢ºä¿æº–ç¢ºæ€§å’Œä¸€è‡´æ€§ã€‚
- åŒ…å«ç›¸é—œçš„ç´°ç¯€ï¼Œå¦‚æ—¥æœŸã€åç¨±ã€è·ä½ç­‰å…·é«”ä¿¡æ¯ï¼Œå¿…è¦æ™‚æä¾›èƒŒæ™¯ä¿¡æ¯ä»¥å¢å¼·ç†è§£ã€‚

3. æ ¼å¼ï¼š
- ä½¿ç”¨ "Q:" æ¨™è¨˜æ¯å€‹å•é¡Œçš„é–‹å§‹
- ä½¿ç”¨ "A:" æ¨™è¨˜å°æ‡‰ç­”æ¡ˆçš„é–‹å§‹
- å•ç­”å°ä¹‹é–“ç”¨å…©å€‹ç©ºè¡Œåˆ†éš”

4. å…§å®¹è¦æ±‚ï¼š
- ç¢ºä¿å•ç­”å°ç·Šå¯†åœç¹æ–‡æœ¬ä¸»é¡Œï¼Œé¿å…åé›¢ä¸»é¡Œã€‚
- é¿å…æ·»åŠ æ–‡æœ¬ä¸­æœªæåŠçš„ä¿¡æ¯ï¼Œç¢ºä¿ä¿¡æ¯çš„çœŸå¯¦æ€§ã€‚

çµ¦å®šæ–‡æœ¬ï¼š
{text_content}

è«‹åŸºæ–¼é€™å€‹æ–‡æœ¬ç”Ÿæˆå¤šå€‹å•ç­”å°ã€‚"""

def get_default_json_system_prompt():
    """ç²å–é»˜èªçš„JSONè½‰æ›ç³»çµ±æç¤ºè©"""
    return """ä½ æ˜¯ä¸€å€‹JSONæ ¼å¼è½‰æ›å°ˆå®¶ã€‚å°‡åŸå§‹å•ç­”å°æ–‡æœ¬è½‰æ›ç‚ºæ¨™æº–JSONæ•¸çµ„ï¼Œæ¯å€‹å•é¡Œå¿…é ˆæˆç‚ºç¨ç«‹çš„QAå°ã€‚

CRITICAL RULES:
- ONLY output valid JSON array format: [...]
- SEPARATE each question into individual QA pairs
- If one "Q:" contains multiple questions, split them into separate objects
- Each question gets its own JSON object with the same answer
- NO explanations, comments, or additional text
- NO markdown code blocks

TASK:
1. Find all Q: and A: pairs
2. If Q: contains multiple questions (separated by newlines), create separate QA pairs for each
3. Each question should be paired with the corresponding answer

EXAMPLE:
If input has: Q: Question1? Question2? Question3? A: Answer content
Output: [
  {"question": "Question1?", "answer": "Answer content"},
  {"question": "Question2?", "answer": "Answer content"},
  {"question": "Question3?", "answer": "Answer content"}
]"""

def test_api_connection(api_key, base_url, model_name):
    """æ¸¬è©¦APIé€£æ¥"""
    try:
        test_client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        
        # ç°¡å–®çš„æ¸¬è©¦è«‹æ±‚
        response = test_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=10,
            temperature=0.1
        )
        
        return True
    except Exception as e:
        st.error(f"APIæ¸¬è©¦å¤±æ•—: {e}")
        return False

def generate_qa_pairs_with_progress(text_chunks):
    """ç”Ÿæˆå•ç­”å°ä¸¦é¡¯ç¤ºé€²åº¦"""
    raw_qa_responses = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ç¬¬ä¸€éšæ®µï¼šç”ŸæˆåŸå§‹QAå°
    status_text.text("ç¬¬ä¸€éšæ®µï¼šç”ŸæˆåŸå§‹QAå°...")
    for i, chunk in enumerate(text_chunks):
        # ä½¿ç”¨è‡ªå®šç¾©çš„QAç”Ÿæˆprompt
        qa_prompt = st.session_state.get('qa_generation_prompt', get_default_qa_prompt())
        prompt = qa_prompt.format(text_content=chunk.page_content)
        
        response = get_completion(prompt)
        if response:
            raw_qa_responses.append({
                "raw_response": response,
                "source_chunk": chunk.page_content
            })
        
        progress = (i + 1) / (len(text_chunks) * 2)  # ç¸½é€²åº¦çš„ä¸€åŠ
        progress_bar.progress(progress)
    
    # ç¬¬äºŒéšæ®µï¼šå°‡åŸå§‹éŸ¿æ‡‰è½‰æ›ç‚ºçµæ§‹åŒ–JSON
    status_text.text("ç¬¬äºŒéšæ®µï¼šè™•ç†ä¸¦çµæ§‹åŒ–QAå°...")
    final_qa_pairs = []
    
    for i, qa_response in enumerate(raw_qa_responses):
        structured_qa_pairs = process_raw_qa_to_json(qa_response["raw_response"], qa_response["source_chunk"])
        final_qa_pairs.extend(structured_qa_pairs)
        
        progress = 0.5 + ((i + 1) / len(raw_qa_responses)) * 0.5  # å¾ŒåŠæ®µé€²åº¦
        progress_bar.progress(progress)
    
    status_text.text(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(final_qa_pairs)} å€‹QAå°")
    return final_qa_pairs

def process_raw_qa_to_json(raw_response, source_chunk):
    """ç›´æ¥ä½¿ç”¨OpenAI APIå°‡åŸå§‹QAéŸ¿æ‡‰è½‰æ›ç‚ºçµæ§‹åŒ–çš„JSONæ ¼å¼"""
    global client
    
    # å¦‚æœå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œå˜—è©¦åˆå§‹åŒ–
    if client is None:
        if not init_openai_client():
            return []
    
    try:
        # ä½¿ç”¨è‡ªå®šç¾©çš„JSONè½‰æ›system prompt
        json_system_prompt = st.session_state.get('json_system_prompt', get_default_json_system_prompt())
        
        # ä½¿ç”¨ç”¨æˆ¶è¨­å®šçš„åƒæ•¸ï¼ŒJSONè½‰æ›å¯ä»¥ä½¿ç”¨ç¨ç«‹çš„æ¨¡å‹
        model = st.session_state.get('json_model_name', st.session_state.get('model_name', 'gpt-4.1-nano'))
        temperature = st.session_state.get('json_temperature', 0.1)
        max_tokens = st.session_state.get('max_tokens', 4096)
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": json_system_prompt
                },
                {
                    "role": "user", 
                    "content": f"INPUT TEXT:\n{raw_response}\n\nJSON OUTPUT:"
                }
            ],
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1
        )
        
        json_response = response.choices[0].message.content.strip()
        
        # ç›´æ¥è§£æJSONï¼Œä¸åšé¡å¤–è™•ç†
        start_bracket = json_response.find('[')
        end_bracket = json_response.rfind(']')
        
        if start_bracket != -1 and end_bracket != -1:
            json_response = json_response[start_bracket:end_bracket + 1]
            qa_list = json.loads(json_response)
            
            # æ·»åŠ source_chunkåˆ°æ¯å€‹QAå°
            for qa in qa_list:
                if isinstance(qa, dict) and 'question' in qa and 'answer' in qa:
                    qa["source_chunk"] = source_chunk
            
            return qa_list
        else:
            st.error("LLMæœªè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼")
            return []
            
    except Exception as e:
        st.error(f"APIèª¿ç”¨å¤±æ•—: {e}")
        return []



def load_single_document(file_path: str) -> List[Document]:
    """è¼‰å…¥å–®å€‹æ–‡æª”"""
    ext = "." + file_path.rsplit(".", 1)[-1]
    if ext in LOADER_MAPPING:
        loader_class, loader_args = LOADER_MAPPING[ext]
        loader = loader_class(file_path, **loader_args)
        return loader.load()
    raise ValueError(f"Unsupported file extension '{ext}'")

def process_files(uploaded_files):
    """è™•ç†ä¸Šå‚³çš„å¤šå€‹æ–‡ä»¶ä¸¦ç”Ÿæˆæ–‡æœ¬å¡Š"""
    all_text_chunks = []
    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        try:
            documents = load_single_document(tmp_file_path)
            if not documents:
                st.error(f"æ–‡ä»¶ {uploaded_file.name} è™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚")
                continue

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)
            text_chunks = text_splitter.split_documents(documents)
            all_text_chunks.extend(text_chunks)
        except Exception as e:
            st.error(f"è™•ç†æ–‡ä»¶ {uploaded_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            os.unlink(tmp_file_path)
    
    return all_text_chunks

def download_qa_pairs_as_json(qa_pairs, filename="qa_pairs.json"):
    """ä¸‹è¼‰QAå°ç‚ºæ¨™æº–JSONæ–‡ä»¶"""
    if qa_pairs:
        json_data = {
            "qa_pairs": qa_pairs,
            "total_count": len(qa_pairs),
            "generated_timestamp": st.session_state.get('generation_timestamp', '')
        }
        
        # æ ¼å¼åŒ–JSONæ•¸æ“šä»¥æé«˜å¯è®€æ€§
        json_str = json.dumps(json_data, ensure_ascii=False, indent=4)
        
        # å‰µå»ºä¸‹è¼‰æŒ‰éˆ•
        st.download_button(
            label="ä¸‹è¼‰QAå°ç‚ºJSONæ–‡ä»¶",
            data=json_str,
            file_name=filename,
            mime="application/json"
        )

def download_qa_pairs_as_sft_format(qa_pairs, system_prompt="ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚", filename="sft_qa_pairs.json"):
    """ä¸‹è¼‰QAå°ç‚ºSFTTraineræ ¼å¼çš„JSONæ–‡ä»¶"""
    if qa_pairs:
        sft_data = []
        for qa in qa_pairs:
            sft_entry = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": qa.get('question', '')},
                    {"role": "assistant", "content": qa.get('answer', '')}
                ]
            }
            sft_data.append(sft_entry)
        
        # æ ¼å¼åŒ–JSONæ•¸æ“š
        json_str = json.dumps(sft_data, ensure_ascii=False, indent=2)
        
        # å‰µå»ºä¸‹è¼‰æŒ‰éˆ•
        st.download_button(
            label="ä¸‹è¼‰SFTTraineræ ¼å¼JSONæ–‡ä»¶",
            data=json_str,
            file_name=filename,
            mime="application/json"
        )

def main():
    """ä¸»å‡½æ•¸ï¼Œè¨­ç½®Streamlitç•Œé¢"""
    st.set_page_config(page_title="QAå°ç”Ÿæˆå™¨", layout="wide")
    st.title("QAå°ç”Ÿæˆå™¨")
    
    st.markdown("### ğŸ“š ä¸Šå‚³æ–‡ä»¶ä¸¦ç”Ÿæˆå•ç­”å°")
    st.markdown("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šTXT, PDF, DOCX, CSV, HTML, Markdownç­‰")
    

    
    # æ–‡ä»¶ä¸Šå‚³
    uploaded_files = st.file_uploader(
        "é¸æ“‡è¦è™•ç†çš„æ–‡ä»¶", 
        type=["txt", "pdf", "docx", "csv", "html", "md", "odt", "ppt", "pptx", "epub", "eml"], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.success(f"å·²ä¸Šå‚³ {len(uploaded_files)} å€‹æ–‡ä»¶ï¼")
        
        # é¡¯ç¤ºä¸Šå‚³çš„æ–‡ä»¶åˆ—è¡¨
        with st.expander("æŸ¥çœ‹ä¸Šå‚³çš„æ–‡ä»¶", expanded=False):
            for file in uploaded_files:
                st.write(f"ğŸ“„ {file.name} ({file.size} bytes)")
        
        if st.button("ğŸš€ é–‹å§‹è™•ç†æ–‡ä»¶ä¸¦ç”ŸæˆQAå°", type="primary"):
            # æª¢æŸ¥APIè¨­å®š
            if not st.session_state.get('api_key'):
                st.error("âŒ è«‹å…ˆåœ¨å´é‚Šæ¬„è¨­å®šAPI Key")
                return
            
            # åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯
            if not init_openai_client():
                st.error("âŒ APIåˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®š")
                return
            
            # è™•ç†æ–‡ä»¶
            with st.spinner("æ­£åœ¨è™•ç†æ–‡ä»¶..."):
                text_chunks = process_files(uploaded_files)
                if not text_chunks:
                    st.error("æ–‡ä»¶è™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚")
                    return
                st.info(f"âœ… æ–‡ä»¶å·²åˆ†å‰²æˆ {len(text_chunks)} å€‹æ–‡æœ¬æ®µ")

            # ç”ŸæˆQAå°
            import time
            st.session_state.generation_timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            st.session_state.qa_pairs = generate_qa_pairs_with_progress(text_chunks)
            if st.session_state.qa_pairs:
                st.success(f"ğŸ‰ ç”Ÿæˆå®Œæˆï¼å…±ç”¢ç”Ÿ {len(st.session_state.qa_pairs)} å€‹ç¨ç«‹çš„QAå°")
                st.info(f"ğŸ’¡ æ¯å€‹æ–‡æœ¬æ®µå¹³å‡ç”¢ç”Ÿ {len(st.session_state.qa_pairs)/len(text_chunks):.1f} å€‹QAå°")
            else:
                st.error("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•QAå°ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æˆ–APIé…ç½®")

    # é¡¯ç¤ºç”Ÿæˆçš„QAå°
    if hasattr(st.session_state, 'qa_pairs') and st.session_state.qa_pairs:
        st.markdown("---")
        st.markdown("### ğŸ“‹ ç”Ÿæˆçš„QAå°")
        
        # çµ±è¨ˆä¿¡æ¯
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½QAå°æ•¸é‡", len(st.session_state.qa_pairs))
        with col2:
            if 'generation_timestamp' in st.session_state:
                st.metric("ç”Ÿæˆæ™‚é–“", st.session_state.generation_timestamp)
        with col3:
            # è¨ˆç®—ä¾†æºæ–‡ä»¶æ•¸é‡
            unique_sources = len(set(qa.get('source_chunk', '')[:100] for qa in st.session_state.qa_pairs))
            st.metric("æ–‡æœ¬æ®µæ•¸é‡", unique_sources)
        with col4:
            st.metric("ä¸‹è¼‰é¸é …", "å¤šç¨®æ ¼å¼")
        
        # ä¸‹è¼‰é¸é …
        st.markdown("#### ğŸ’¾ ä¸‹è¼‰é¸é …")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            download_qa_pairs_as_json(st.session_state.qa_pairs)
        
        with col2:
            # SFTæ ¼å¼ä¸‹è¼‰è¨­å®š
            with st.expander("SFTæ ¼å¼è¨­å®š", expanded=False):
                sft_system_prompt = st.text_area(
                    "System Prompt",
                    value=st.session_state.get('sft_system_prompt', 'ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚'),
                    height=100,
                    help="ç”¨æ–¼SFTè¨“ç·´çš„ç³»çµ±æç¤ºè©"
                )
                st.session_state.sft_system_prompt = sft_system_prompt
            
            download_qa_pairs_as_sft_format(
                st.session_state.qa_pairs, 
                system_prompt=st.session_state.get('sft_system_prompt', 'ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚')
            )
        
        with col3:
            st.markdown("**æ ¼å¼èªªæ˜ï¼š**")
            st.markdown("â€¢ æ¨™æº–JSONï¼šåŸå§‹QAå°æ ¼å¼")
            st.markdown("â€¢ SFTæ ¼å¼ï¼šé©ç”¨æ–¼æ¨¡å‹å¾®èª¿")
        
        # QAå°é è¦½
        st.markdown("#### ğŸ” QAå°é è¦½")
        
        # åˆ†é é¡¯ç¤º
        items_per_page = 5
        total_pages = (len(st.session_state.qa_pairs) + items_per_page - 1) // items_per_page
        
        if total_pages > 1:
            page = st.selectbox("é¸æ“‡é é¢", range(1, total_pages + 1), key="page_selector")
            start_idx = (page - 1) * items_per_page
            end_idx = min(start_idx + items_per_page, len(st.session_state.qa_pairs))
        else:
            start_idx = 0
            end_idx = len(st.session_state.qa_pairs)
        
        # é¡¯ç¤ºç•¶å‰é çš„QAå°
        for i in range(start_idx, end_idx):
            qa = st.session_state.qa_pairs[i]
            with st.expander(f"**QAå° {start_idx + i + 1}**", expanded=False):
                st.markdown("**â“ å•é¡Œ:**")
                st.markdown(qa['question'])
                st.markdown("**âœ… ç­”æ¡ˆ:**")
                st.markdown(qa['answer'])
                if 'source_chunk' in qa:
                    st.markdown("**ğŸ“„ ä¾†æºæ–‡æœ¬:**")
                    st.text_area(
                        "ä¾†æºæ–‡æœ¬å…§å®¹", 
                        value=qa['source_chunk'], 
                        height=100, 
                        key=f"source_{start_idx + i}", 
                        disabled=True,
                        label_visibility="collapsed"
                    )
    
    # å´é‚Šæ¬„ä¿¡æ¯
    with st.sidebar:
        st.markdown("### ğŸ”‘ APIè¨­å®š")
        
        # APIè¨­å®šåŠŸèƒ½
        with st.expander("ğŸ”§ API & æ¨¡å‹è¨­å®š", expanded=True):
            # API Keyè¨­å®š
            api_key = st.text_input(
                "OpenAI API Key",
                value=st.session_state.get('api_key', ''),
                type="password",
                help="è«‹è¼¸å…¥æ‚¨çš„OpenAI API Key"
            )
            
            # Base URLè¨­å®š
            base_url = st.text_input(
                "Base URL",
                value=st.session_state.get('base_url', 'https://api.openai.com/v1'),
                help="APIçš„åŸºç¤URLï¼Œé»˜èªç‚ºOpenAIå®˜æ–¹"
            )
            
            # æ¨¡å‹è¨­å®š
            model_name = st.text_input(
                "QAç”Ÿæˆæ¨¡å‹åç¨±",
                value=st.session_state.get('model_name', 'gpt-4.1-nano'),
                help="ç”¨æ–¼ç”ŸæˆQAå°çš„æ¨¡å‹åç¨±"
            )
            
            # JSONè½‰æ›å°ˆç”¨æ¨¡å‹è¨­å®š
            json_model_name = st.text_input(
                "JSONè½‰æ›æ¨¡å‹åç¨±",
                value=st.session_state.get('json_model_name', ''),
                help="ç”¨æ–¼JSONè½‰æ›çš„æ¨¡å‹åç¨±ï¼Œç•™ç©ºå‰‡ä½¿ç”¨QAç”Ÿæˆæ¨¡å‹"
            )
            
            # åƒæ•¸è¨­å®š
            col1, col2 = st.columns(2)
            with col1:
                temperature = st.slider(
                    "QAç”ŸæˆTemperature",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get('temperature', 0.1),
                    step=0.1,
                    help="æ§åˆ¶ç”Ÿæˆçš„éš¨æ©Ÿæ€§"
                )
            
            with col2:
                json_temperature = st.slider(
                    "JSONè½‰æ›Temperature",
                    min_value=0.0,
                    max_value=1.0,
                    value=st.session_state.get('json_temperature', 0.1),
                    step=0.1,
                    help="JSONè½‰æ›çš„æº«åº¦åƒæ•¸"
                )
            
            max_tokens = st.number_input(
                "æœ€å¤§Tokenæ•¸",
                min_value=100,
                max_value=32000,
                value=st.session_state.get('max_tokens', 4096),
                step=100,
                help="æ¯æ¬¡APIèª¿ç”¨çš„æœ€å¤§tokenæ•¸"
            )
            
            # ä¿å­˜è¨­å®šæŒ‰éˆ•
            if st.button("ğŸ’¾ ä¿å­˜APIè¨­å®š", key="save_api_settings"):
                st.session_state.api_key = api_key
                st.session_state.base_url = base_url
                st.session_state.model_name = model_name
                st.session_state.json_model_name = json_model_name if json_model_name else model_name
                st.session_state.temperature = temperature
                st.session_state.json_temperature = json_temperature
                st.session_state.max_tokens = max_tokens
                
                # é‡æ–°åˆå§‹åŒ–å®¢æˆ¶ç«¯
                global client
                client = None
                if init_openai_client():
                    st.success("âœ… APIè¨­å®šå·²ä¿å­˜ä¸¦æ¸¬è©¦æˆåŠŸ")
                else:
                    st.error("âŒ APIè¨­å®šä¿å­˜å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®š")
            
            # æ¸¬è©¦é€£æ¥æŒ‰éˆ•
            if st.button("ğŸ”Œ æ¸¬è©¦APIé€£æ¥", key="test_api_connection"):
                if api_key:
                    # æ¸¬è©¦QAç”Ÿæˆæ¨¡å‹
                    test_result_qa = test_api_connection(api_key, base_url, model_name)
                    if test_result_qa:
                        st.success("âœ… QAç”Ÿæˆæ¨¡å‹é€£æ¥æ¸¬è©¦æˆåŠŸ")
                        
                        # å¦‚æœè¨­å®šäº†ä¸åŒçš„JSONè½‰æ›æ¨¡å‹ï¼Œä¹Ÿé€²è¡Œæ¸¬è©¦
                        if json_model_name and json_model_name != model_name:
                            test_result_json = test_api_connection(api_key, base_url, json_model_name)
                            if test_result_json:
                                st.success("âœ… JSONè½‰æ›æ¨¡å‹é€£æ¥æ¸¬è©¦æˆåŠŸ")
                            else:
                                st.error("âŒ JSONè½‰æ›æ¨¡å‹é€£æ¥æ¸¬è©¦å¤±æ•—")
                    else:
                        st.error("âŒ QAç”Ÿæˆæ¨¡å‹é€£æ¥æ¸¬è©¦å¤±æ•—")
                else:
                    st.warning("âš ï¸ è«‹å…ˆè¼¸å…¥API Key")
        
        st.markdown("---")
        st.markdown("### âš™ï¸ æç¤ºè©è¨­å®š")
        
        # æç¤ºè©è‡ªå®šç¾©åŠŸèƒ½
        with st.expander("ğŸ”§ è‡ªå®šç¾©æç¤ºè©", expanded=False):
            st.markdown("#### ç¬¬ä¸€éšæ®µï¼šQAç”Ÿæˆæç¤ºè©")
            
            # åˆå§‹åŒ–é»˜èªæç¤ºè©
            if 'qa_generation_prompt' not in st.session_state:
                st.session_state.qa_generation_prompt = get_default_qa_prompt()
            
            # QAç”Ÿæˆæç¤ºè©ç·¨è¼¯å™¨
            qa_prompt = st.text_area(
                "QAç”Ÿæˆæç¤ºè© (ä½¿ç”¨ {text_content} ä½œç‚ºæ–‡æœ¬å…§å®¹çš„å ä½ç¬¦)",
                value=st.session_state.qa_generation_prompt,
                height=200,
                key="qa_prompt_editor"
            )
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ’¾ ä¿å­˜QAæç¤ºè©", key="save_qa_prompt"):
                    st.session_state.qa_generation_prompt = qa_prompt
                    st.success("âœ… QAæç¤ºè©å·²ä¿å­˜")
            
            with col2:
                if st.button("ğŸ”„ é‡ç½®QAæç¤ºè©", key="reset_qa_prompt"):
                    st.session_state.qa_generation_prompt = get_default_qa_prompt()
                    st.success("âœ… QAæç¤ºè©å·²é‡ç½®")
                    st.rerun()
            
            st.markdown("---")
            st.markdown("#### ç¬¬äºŒéšæ®µï¼šJSONè½‰æ›æç¤ºè©")
            
            # åˆå§‹åŒ–é»˜èªJSONæç¤ºè©
            if 'json_system_prompt' not in st.session_state:
                st.session_state.json_system_prompt = get_default_json_system_prompt()
            
            # JSONè½‰æ›æç¤ºè©ç·¨è¼¯å™¨
            json_prompt = st.text_area(
                "JSONè½‰æ›ç³»çµ±æç¤ºè©",
                value=st.session_state.json_system_prompt,
                height=200,
                key="json_prompt_editor"
            )
            
            col3, col4 = st.columns(2)
            with col3:
                if st.button("ğŸ’¾ ä¿å­˜JSONæç¤ºè©", key="save_json_prompt"):
                    st.session_state.json_system_prompt = json_prompt
                    st.success("âœ… JSONæç¤ºè©å·²ä¿å­˜")
            
            with col4:
                if st.button("ğŸ”„ é‡ç½®JSONæç¤ºè©", key="reset_json_prompt"):
                    st.session_state.json_system_prompt = get_default_json_system_prompt()
                    st.success("âœ… JSONæç¤ºè©å·²é‡ç½®")
                    st.rerun()
        
        st.markdown("---")
        st.markdown("### â„¹ï¸ ä½¿ç”¨èªªæ˜")
        st.markdown("""
        **ğŸ“‹ è™•ç†æµç¨‹ï¼š**
        1. ä¸Šå‚³ä¸€å€‹æˆ–å¤šå€‹æ–‡ä»¶
        2. é»æ“Š"é–‹å§‹è™•ç†"æŒ‰éˆ•
        3. ç³»çµ±å°‡è‡ªå‹•ï¼š
           - åˆ†å‰²æ–‡æœ¬ç‚ºå°æ®µ
           - ç”ŸæˆåŸå§‹QAå°
           - ç”¨LLMè½‰æ›ç‚ºJSONæ ¼å¼
           - åˆ†é›¢æˆç¨ç«‹çš„QAå°
        4. æŸ¥çœ‹å’Œä¸‹è¼‰ç”Ÿæˆçš„QAå°
        
        **ğŸ—‚ï¸ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š**
        - æ–‡æœ¬æ–‡ä»¶ (.txt)
        - PDFæ–‡ä»¶ (.pdf)  
        - Wordæ–‡æª” (.docx)
        - CSVæ–‡ä»¶ (.csv)
        - HTMLæ–‡ä»¶ (.html)
        - Markdownæ–‡ä»¶ (.md)
        - PowerPoint (.ppt, .pptx)
        - é›»å­éƒµä»¶ (.eml)
        - é›»å­æ›¸ (.epub)
        - ç­‰ç­‰...
        """)
        
        if hasattr(st.session_state, 'qa_pairs') and st.session_state.qa_pairs:
            st.markdown("---")
            st.markdown("### ğŸ“Š ç”Ÿæˆçµ±è¨ˆ")
            st.write(f"ğŸ“ QAå°ç¸½æ•¸: **{len(st.session_state.qa_pairs)}**")
            if 'generation_timestamp' in st.session_state:
                st.write(f"â° ç”Ÿæˆæ™‚é–“: **{st.session_state.generation_timestamp}**")
            
            # çµ±è¨ˆå•é¡Œé¡å‹
            question_types = {}
            for qa in st.session_state.qa_pairs:
                question = qa.get('question', '').strip()
                if question.startswith('ä»€éº¼'):
                    question_types['ä»€éº¼'] = question_types.get('ä»€éº¼', 0) + 1
                elif question.startswith('å¦‚ä½•') or question.startswith('æ€æ¨£'):
                    question_types['å¦‚ä½•/æ€æ¨£'] = question_types.get('å¦‚ä½•/æ€æ¨£', 0) + 1
                elif question.startswith('ç‚ºä»€éº¼'):
                    question_types['ç‚ºä»€éº¼'] = question_types.get('ç‚ºä»€éº¼', 0) + 1
                elif 'ï¼Ÿ' in question:
                    question_types['å…¶ä»–å•å¥'] = question_types.get('å…¶ä»–å•å¥', 0) + 1
                else:
                    question_types['é™³è¿°å¼'] = question_types.get('é™³è¿°å¼', 0) + 1
            
            if question_types:
                st.markdown("**â“ å•é¡Œé¡å‹åˆ†å¸ƒ:**")
                for q_type, count in question_types.items():
                    st.write(f"  - {q_type}: {count}")
                
                st.markdown("**ğŸ”„ è™•ç†èªªæ˜:**")
                st.write("- ç›´æ¥ä½¿ç”¨OpenAI APIè™•ç†")
                st.write("- LLMè‡ªå‹•åˆ†é›¢åˆä½µå•é¡Œ")
                st.write("- ç´”JSONæ ¼å¼è¼¸å‡º")
                
                st.markdown("**âš™ï¸ ç•¶å‰è¨­å®š:**")
                st.write(f"- QAç”Ÿæˆæ¨¡å‹: {st.session_state.get('model_name', 'gpt-4.1-nano')}")
                json_model = st.session_state.get('json_model_name', st.session_state.get('model_name', 'gpt-4.1-nano'))
                st.write(f"- JSONè½‰æ›æ¨¡å‹: {json_model}")
                st.write(f"- QAæº«åº¦: {st.session_state.get('temperature', 0.1)}")
                st.write(f"- JSONæº«åº¦: {st.session_state.get('json_temperature', 0.1)}")
                st.write(f"- æœ€å¤§Token: {st.session_state.get('max_tokens', 4096)}")

if __name__ == "__main__":
    main()